{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP_TASK.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ij99q8ZDbns_",
        "colab_type": "code",
        "outputId": "b92acdc1-77c0-4d73-efbd-bf4058666719",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "source": [
        "! pip install transformers"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (2.9.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.4)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: tokenizers==0.7.0 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.86)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.4.5.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.9)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6bsiYTKtbnqJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "from typing import Tuple, List\n",
        "from functools import partial\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, RandomSampler\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from transformers import BertTokenizer, BertModel, AdamW, get_linear_schedule_with_warmup, BertPreTrainedModel\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from tqdm import tqdm\n",
        "import itertools"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S1dl0LRqbnn_",
        "colab_type": "code",
        "outputId": "09739f8b-2c46-4886-f7e9-a38a1698436a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wYzxmxHebnkQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "url = '/content/gdrive/My Drive/jigsaw-toxic-comment-classification-challenge/'\n",
        "\n",
        "df = pd.read_csv(os.path.join(url,'train.csv'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "og8bljHyYqyh",
        "colab_type": "text"
      },
      "source": [
        "The model taking too much time hence taking a fraction\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5TiqBt2u9uyd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_df = df[:1200]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lrH30k21bnfG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device = torch.device('cpu')\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device('cuda:0')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4EKkZ5cXbnYD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "evJJ8VcwdCvH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_df, val_df = train_test_split(train_df, test_size=0.2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SpL6aZ0KckAa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ToxicDataset(Dataset):\n",
        "    \n",
        "    def __init__(self, tokenizer: BertTokenizer, dataframe: pd.DataFrame, lazy: bool = False):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.pad_idx = tokenizer.pad_token_id\n",
        "        self.lazy = lazy\n",
        "        if not self.lazy:\n",
        "            self.X = []\n",
        "            self.Y = []\n",
        "            for i, (row) in tqdm(dataframe.iterrows()):\n",
        "                x, y = self.row_to_tensor(self.tokenizer, row)\n",
        "                self.X.append(x)\n",
        "                self.Y.append(y)\n",
        "        else:\n",
        "            self.df = dataframe        \n",
        "    \n",
        "    @staticmethod\n",
        "    def row_to_tensor(tokenizer: BertTokenizer, row: pd.Series) -> Tuple[torch.LongTensor, torch.LongTensor]:\n",
        "        tokens = tokenizer.encode(row[\"comment_text\"], add_special_tokens=True)\n",
        "        if len(tokens) > 512:\n",
        "            tokens = tokens[:511] + [tokens[-1]]\n",
        "        x = torch.LongTensor(tokens)\n",
        "        y = torch.FloatTensor(row[[\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]])\n",
        "        return x, y\n",
        "        \n",
        "    \n",
        "    def __len__(self):\n",
        "        if self.lazy:\n",
        "            return len(self.df)\n",
        "        else:\n",
        "            return len(self.X)\n",
        "\n",
        "    def __getitem__(self, index: int) -> Tuple[torch.LongTensor, torch.LongTensor]:\n",
        "        if not self.lazy:\n",
        "            return self.X[index], self.Y[index]\n",
        "        else:\n",
        "            return self.row_to_tensor(self.tokenizer, self.df.iloc[index])\n",
        "            \n",
        "\n",
        "def collate_fn(batch: List[Tuple[torch.LongTensor, torch.LongTensor]], device: torch.device) \\\n",
        "        -> Tuple[torch.LongTensor, torch.LongTensor]:\n",
        "    x, y = list(zip(*batch))\n",
        "    x = pad_sequence(x, batch_first=True, padding_value=0)\n",
        "    y = torch.stack(y)\n",
        "    return x.to(device), y.to(device)\n",
        "\n",
        "train_dataset = ToxicDataset(tokenizer, train_df, lazy=True)\n",
        "dev_dataset = ToxicDataset(tokenizer, val_df, lazy=True)\n",
        "collate_fn = partial(collate_fn, device=device)\n",
        "BATCH_SIZE = 10\n",
        "train_sampler = RandomSampler(train_dataset)\n",
        "dev_sampler = RandomSampler(dev_dataset)\n",
        "train_iterator = DataLoader(train_dataset, batch_size=BATCH_SIZE, sampler=train_sampler, collate_fn=collate_fn)\n",
        "dev_iterator = DataLoader(dev_dataset, batch_size=BATCH_SIZE, sampler=dev_sampler, collate_fn=collate_fn)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gF8HkwsmdI40",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BertClassifier(nn.Module):\n",
        "    \n",
        "    def __init__(self, bert: BertModel, num_classes: int):\n",
        "        super().__init__()\n",
        "        self.bert = bert\n",
        "        self.classifier = nn.Linear(bert.config.hidden_size, num_classes)\n",
        "        \n",
        "    def forward(self, input_ids, attention_mask=None, token_type_ids=None, position_ids=None, head_mask=None,labels=None):\n",
        "        outputs = self.bert(input_ids,\n",
        "                               attention_mask=attention_mask,\n",
        "                               token_type_ids=token_type_ids,\n",
        "                               position_ids=position_ids,\n",
        "                               head_mask=head_mask)\n",
        "        cls_output = outputs[1] # batch, hidden\n",
        "        cls_output = self.classifier(cls_output) # batch, 6\n",
        "        cls_output = torch.sigmoid(cls_output)\n",
        "        criterion = nn.BCELoss()\n",
        "        loss = 0\n",
        "        if labels is not None:\n",
        "            loss = criterion(cls_output, labels)\n",
        "        return loss, cls_output\n",
        "\n",
        "model = BertClassifier(BertModel.from_pretrained('bert-base-uncased'), 6).to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UDe9nCWrdcA_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(model, iterator, optimizer, scheduler):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for x, y in tqdm(iterator,position=0, leave=True):\n",
        "        optimizer.zero_grad()\n",
        "        mask = (x != 0).float()\n",
        "        loss, outputs = model(x, attention_mask=mask, labels=y)\n",
        "        total_loss += loss.item()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "    print(\"Train loss:\", total_loss / len(iterator))\n",
        "\n",
        "def evaluate(model, iterator):\n",
        "    model.eval()\n",
        "    pred = []\n",
        "    true = []\n",
        "    with torch.no_grad():\n",
        "        total_loss = 0\n",
        "        for x, y in tqdm(iterator,position=0, leave=True):\n",
        "            mask = (x != 0).float()\n",
        "            loss, outputs = model(x, attention_mask=mask, labels=y)\n",
        "            total_loss += loss\n",
        "            true += y.cpu().numpy().tolist()\n",
        "            pred += outputs.cpu().numpy().tolist()\n",
        "    true = np.array(true)\n",
        "    pred = np.array(pred)\n",
        "    for i, name in enumerate(['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']):\n",
        "        print(name, \"roc_auc\" ,roc_auc_score(true[:, i], pred[:, i]))\n",
        "    print(\"Evaluate loss:\",  total_loss / len(iterator))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p4ZDaxE8d0um",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "optimizer = AdamW(model.parameters(),lr = 2e-5,eps = 1e-8)\n",
        "\n",
        "epochs = 2\n",
        "total_steps = len(train_iterator) * epochs\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                            num_warmup_steps = 0, \n",
        "                                            num_training_steps = total_steps)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u5_YGMWQd1D7",
        "colab_type": "code",
        "outputId": "2c6cdeca-83bb-4d87-beb2-73f49859ae8f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "for i in range(epochs):\n",
        "  train(model, train_iterator, optimizer, scheduler)\n",
        "  evaluate(model, dev_iterator)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 0/96 [00:00<?, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (852 > 512). Running this sequence through the model will result in indexing errors\n",
            "  1%|          | 1/96 [00:01<02:47,  1.76s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (648 > 512). Running this sequence through the model will result in indexing errors\n",
            "  9%|▉         | 9/96 [00:09<01:33,  1.08s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (537 > 512). Running this sequence through the model will result in indexing errors\n",
            " 12%|█▎        | 12/96 [00:11<01:13,  1.15it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (931 > 512). Running this sequence through the model will result in indexing errors\n",
            " 14%|█▎        | 13/96 [00:13<01:32,  1.12s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (1088 > 512). Running this sequence through the model will result in indexing errors\n",
            " 15%|█▍        | 14/96 [00:15<01:45,  1.28s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (540 > 512). Running this sequence through the model will result in indexing errors\n",
            " 21%|██        | 20/96 [00:21<01:13,  1.03it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (877 > 512). Running this sequence through the model will result in indexing errors\n",
            " 23%|██▎       | 22/96 [00:23<01:12,  1.03it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (675 > 512). Running this sequence through the model will result in indexing errors\n",
            " 34%|███▍      | 33/96 [00:36<01:12,  1.15s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (628 > 512). Running this sequence through the model will result in indexing errors\n",
            " 47%|████▋     | 45/96 [00:47<00:44,  1.15it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (800 > 512). Running this sequence through the model will result in indexing errors\n",
            " 53%|█████▎    | 51/96 [00:53<00:36,  1.25it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (839 > 512). Running this sequence through the model will result in indexing errors\n",
            " 61%|██████▏   | 59/96 [01:00<00:26,  1.37it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (629 > 512). Running this sequence through the model will result in indexing errors\n",
            " 68%|██████▊   | 65/96 [01:05<00:22,  1.36it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (950 > 512). Running this sequence through the model will result in indexing errors\n",
            " 71%|███████   | 68/96 [01:07<00:18,  1.51it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (1227 > 512). Running this sequence through the model will result in indexing errors\n",
            " 77%|███████▋  | 74/96 [01:13<00:19,  1.13it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (730 > 512). Running this sequence through the model will result in indexing errors\n",
            " 91%|█████████ | 87/96 [01:26<00:06,  1.39it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (795 > 512). Running this sequence through the model will result in indexing errors\n",
            " 92%|█████████▏| 88/96 [01:28<00:08,  1.02s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (810 > 512). Running this sequence through the model will result in indexing errors\n",
            "100%|██████████| 96/96 [01:36<00:00,  1.00s/it]\n",
            "  0%|          | 0/24 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss: 0.2979417548825343\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 12%|█▎        | 3/24 [00:00<00:04,  4.49it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (1019 > 512). Running this sequence through the model will result in indexing errors\n",
            " 29%|██▉       | 7/24 [00:02<00:04,  3.58it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (658 > 512). Running this sequence through the model will result in indexing errors\n",
            " 46%|████▌     | 11/24 [00:03<00:04,  2.94it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (512 > 512). Running this sequence through the model will result in indexing errors\n",
            " 67%|██████▋   | 16/24 [00:05<00:02,  2.91it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (674 > 512). Running this sequence through the model will result in indexing errors\n",
            " 75%|███████▌  | 18/24 [00:06<00:02,  2.89it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (1052 > 512). Running this sequence through the model will result in indexing errors\n",
            " 79%|███████▉  | 19/24 [00:06<00:02,  2.29it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (1060 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (1127 > 512). Running this sequence through the model will result in indexing errors\n",
            "100%|██████████| 24/24 [00:08<00:00,  2.77it/s]\n",
            "  0%|          | 0/96 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "toxic roc_auc 0.9131631631631631\n",
            "severe_toxic roc_auc 1.0\n",
            "obscene roc_auc 0.915954415954416\n",
            "threat roc_auc 0.8326359832635983\n",
            "insult roc_auc 0.8874643874643875\n",
            "identity_hate roc_auc 0.9705882352941176\n",
            "Evaluate loss: tensor(0.1195, device='cuda:0')\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  4%|▍         | 4/96 [00:02<01:11,  1.29it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (648 > 512). Running this sequence through the model will result in indexing errors\n",
            "  7%|▋         | 7/96 [00:05<01:16,  1.16it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (1227 > 512). Running this sequence through the model will result in indexing errors\n",
            " 10%|█         | 10/96 [00:08<01:12,  1.18it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (629 > 512). Running this sequence through the model will result in indexing errors\n",
            " 11%|█▏        | 11/96 [00:10<01:33,  1.10s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (950 > 512). Running this sequence through the model will result in indexing errors\n",
            " 18%|█▊        | 17/96 [00:15<00:53,  1.48it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (877 > 512). Running this sequence through the model will result in indexing errors\n",
            " 36%|███▋      | 35/96 [00:33<00:53,  1.14it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (931 > 512). Running this sequence through the model will result in indexing errors\n",
            " 44%|████▍     | 42/96 [00:40<00:45,  1.18it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (675 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (1088 > 512). Running this sequence through the model will result in indexing errors\n",
            " 48%|████▊     | 46/96 [00:44<00:49,  1.01it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (800 > 512). Running this sequence through the model will result in indexing errors\n",
            " 55%|█████▌    | 53/96 [00:49<00:27,  1.59it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (540 > 512). Running this sequence through the model will result in indexing errors\n",
            " 64%|██████▎   | 61/96 [00:59<00:32,  1.08it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (628 > 512). Running this sequence through the model will result in indexing errors\n",
            " 69%|██████▉   | 66/96 [01:02<00:20,  1.46it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (730 > 512). Running this sequence through the model will result in indexing errors\n",
            " 71%|███████   | 68/96 [01:06<00:31,  1.14s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (839 > 512). Running this sequence through the model will result in indexing errors\n",
            " 81%|████████▏ | 78/96 [01:15<00:15,  1.16it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (537 > 512). Running this sequence through the model will result in indexing errors\n",
            " 90%|████████▉ | 86/96 [01:23<00:10,  1.05s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (795 > 512). Running this sequence through the model will result in indexing errors\n",
            " 93%|█████████▎| 89/96 [01:27<00:06,  1.00it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (852 > 512). Running this sequence through the model will result in indexing errors\n",
            " 99%|█████████▉| 95/96 [01:32<00:00,  1.19it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (810 > 512). Running this sequence through the model will result in indexing errors\n",
            "100%|██████████| 96/96 [01:34<00:00,  1.02it/s]\n",
            "  0%|          | 0/24 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss: 0.12685720385828367\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  4%|▍         | 1/24 [00:00<00:08,  2.68it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (1019 > 512). Running this sequence through the model will result in indexing errors\n",
            " 21%|██        | 5/24 [00:02<00:08,  2.37it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (658 > 512). Running this sequence through the model will result in indexing errors\n",
            " 58%|█████▊    | 14/24 [00:05<00:02,  3.57it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (512 > 512). Running this sequence through the model will result in indexing errors\n",
            " 75%|███████▌  | 18/24 [00:06<00:01,  3.04it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (1127 > 512). Running this sequence through the model will result in indexing errors\n",
            " 83%|████████▎ | 20/24 [00:07<00:01,  2.75it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (1052 > 512). Running this sequence through the model will result in indexing errors\n",
            " 92%|█████████▏| 22/24 [00:08<00:00,  2.76it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (1060 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (674 > 512). Running this sequence through the model will result in indexing errors\n",
            "100%|██████████| 24/24 [00:09<00:00,  2.59it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "toxic roc_auc 0.9582082082082082\n",
            "severe_toxic roc_auc 0.99581589958159\n",
            "obscene roc_auc 0.9736467236467237\n",
            "threat roc_auc 0.9414225941422594\n",
            "insult roc_auc 0.9537037037037037\n",
            "identity_hate roc_auc 1.0\n",
            "Evaluate loss: tensor(0.0929, device='cuda:0')\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4kwADiOHhAqz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}